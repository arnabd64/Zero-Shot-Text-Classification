{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Quantization\n",
    "\n",
    "In this notebook, we shall use __ONNX__ to quantize our model and then upload the quantized model to our huggingface repository before deploying it. The process of quantization involves reducing the precision of the weights of the model from `float32` to some lower precision like `float16` or `int8`. This can reduce the overall memory footprint of the model and makes the execution faster. There are a number of quantization techniques available in the market but we are choosing to use a simple method provided by [Huggingface Optimum](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models) Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification, ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Login to Huggingface\n",
    "\n",
    "To upload the model to huggingface, you need a Huggingface Access Token that has Write Permissions enabled. To get your own token follow the steps:\n",
    "\n",
    "1. In a web browser, login to your [Huggingface](https://hf.co/) Account.\n",
    "2. Click on your profile Picture and Go to Settings.\n",
    "3. Go to __Access Tokens__ and Select __Create new Token__.\n",
    "4. Select __Write__ option under Token Type and provide a name for your key.\n",
    "5. Select __Create token__ to generate the token. Copy the token and paste in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"Paste-your-Token\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the model to Quantize\n",
    "\n",
    "From the Huggingface model repository, choose a model that can be loaded with the `AutoModelForSequenceClassification` class and paste it's repo id in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to use\n",
    "BASE_PYTORCH_MODEL = \"sileod/deberta-v3-base-tasksource-nli\"\n",
    "\n",
    "# Path to quantized models\n",
    "ONNX_PATH = Path(\"onnx\")\n",
    "QUANTIZED_MODEL = Path(\"quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Quantization Configuration\n",
    "\n",
    "Here we will be applying __Dynamic Quantization__ to the model, which is reducing the precision of the weights of the model beforehand and quantizing the activation nodes dynamically during the inference process. Since our base model is a Pytorch model, it first has to be converted to ONNX format and then quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pytorch model to ONNX\n",
    "onnx_model = ORTModelForSequenceClassification.from_pretrained(BASE_PYTORCH_MODEL, export=True, token=HF_TOKEN)\n",
    "\n",
    "# quantize the model\n",
    "dynamic_quantization = AutoQuantizationConfig.avx2(is_static=False)\n",
    "quantizer = ORTQuantizer.from_pretrained(onnx_model)\n",
    "quantizer.quantize(dynamic_quantization, save_dir=QUANTIZED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push the model to Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model = ORTModelForSequenceClassification.from_pretrained(QUANTIZED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.push_to_hub(\n",
    "    QUANTIZED_MODEL,\n",
    "    \"pitangent-ds/deberta-v3-nli-onnx-quantized\",\n",
    "    token=HF_TOKEN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
